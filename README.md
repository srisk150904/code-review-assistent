# ğŸš€ Code Review Assistant

An automated **code review system** that analyzes uploaded code files or pasted snippets for **readability, modularity, and potential bugs**, combining **static analysis** + **LLM suggestions**.

This project is designed to showcase **API design, code analysis, LLM integration, and a working frontend**.

---

## ğŸŒ Live Demo

ğŸ‘‰ **Frontend (Streamlit UI):**
ğŸ”— [https://your-streamlit-app.streamlit.app]([https://your-streamlit-app.streamlit.app](https://code-review-assistent-qvuhqoctvu65sxxxszt2uy.streamlit.app/))

ğŸ‘‰ **Backend (FastAPI API Docs):**
ğŸ”— [https://code-review-assistent.onrender.com/docs](https://code-review-assistent.onrender.com/docs)

*(For evaluators: You can directly use the Streamlit app above. The backend API is deployed separately on Render and powers the UI.)*

---

## ğŸ“‚ Project Structure

```
code-review-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py              # FastAPI app (main entry)
â”‚   â”œâ”€â”€ llm.py              # LLM integration (mock/real)
â”‚   â”œâ”€â”€ analyzers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ python_static.py # Python AST static analyzer
â”‚   â”œâ”€â”€ models.py           # SQLModel schema + DB helpers
â”‚   â”œâ”€â”€ schemas.py          # Request/response schemas
â”‚   â”œâ”€â”€ settings.py         # Config & environment variables
â”‚   â””â”€â”€ utils.py            # File handling utilities
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ sample.py           # Example test file
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_api_smoke.py   # Simple test scaffold
â”œâ”€â”€ frontend.py             # Streamlit UI
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example            # Environment template
â””â”€â”€ README.md               # Documentation
```

---

## âš¡ Requirements (for local run)

* **Python 3.10+**
* **pip**
* (Optional) **OpenAI API key** for real LLM reviews
  *(By default, a mock reviewer is used so it works offline too.)*

---

## â–¶ï¸ Running Locally

### 1. Backend (FastAPI)

Start backend API:

```bash
uvicorn backend.app:app --reload
```

Swagger UI â†’ [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

### 2. Frontend (Streamlit)

Run frontend:

```bash
streamlit run frontend.py
```

UI â†’ [http://localhost:8501](http://localhost:8501)

---

## ğŸ¯ How It Works

1. **Input**: Upload Python file or paste code snippet.
2. **Static Analysis**:

   * Parses code using AST
   * Finds missing docstrings, large functions, cyclomatic complexity, style issues
3. **LLM Suggestions**:

   * If `OPENAI_API_KEY` is provided â†’ sends code + findings to LLM
   * Else â†’ returns mock suggestions for demo
4. **Storage**:

   * Results stored in SQLite (`reviews.db`)
5. **Output**: JSON + Streamlit dashboard with:

   * Score
   * Summary
   * Suggestions
   * Static findings

---

## âœ… Example Workflow

1. Open the **Streamlit app**:
   ğŸ‘‰ [https://your-streamlit-app.streamlit.app](https://your-streamlit-app.streamlit.app)

2. Upload `examples/sample.py`.

3. Get:

   * Score (0â€“100) with progress bar
   * Readability/bug/modularity suggestions
   * Static analysis findings

4. View past reports in the UI.

---

## ğŸ“Œ Notes

* Works offline with mock suggestions.
* Add your `OPENAI_API_KEY` in `.env` for real LLM integration.
* Backend is deployed on Render (ephemeral SQLite DB).
* Frontend is deployed on Streamlit Cloud for easy access.

---

âœ¨ With this setup, an evaluator can:

* Use the **live Streamlit demo** directly
* Or run backend + frontend locally
* Clearly see how uploaded code â†’ review report pipeline works

---
